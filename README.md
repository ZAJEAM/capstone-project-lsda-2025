<!-- PROJECT HEADER -->
<div align="center">
  
![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)

  <h3 align="center">AWS Data Engineering Capstone Project</h3>

  <p align="center">
    Lecturers: Szabó Ildikó Borbásné, Bálint Mátyus
    <br />
    Course: Large Scale Data Architectures
    <br />
    Student: Tussipbek Madi
    <br />
    December, 2025
  </p>
</div>


<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#project-goal">Project Goal</a></li>
    <li><a href="#key-objectives">Key Objectives</a></li>
    <li><a href="#architecture-overview">Architecture Overview</a>
      <ul>
        <li><a href="#starting-architecture">Starting Architecture</a></li>
        <li><a href="#supplementary-tools-used">Supplementary Tools Used</a></li>
        <li><a href="#final-architecture">Final Architecture</a></li>
      </ul>
    </li>
  </ol>
</details>

## Project Goal

The goal of this capstone project is to design and implement an end-to-end data engineering solution on AWS that allows fisheries data to be stored, cataloged, and analyzed at scale using managed cloud services.

### Key Objectives

- Transform raw CSV files into optimized Parquet format  
- Automate schema discovery and metadata management with AWS Glue  
- Enable SQL-based analytics using Amazon Athena  
- Create reusable views for analytical queries  

---
## Architecture Overview

The solution is built using the following AWS services:
- [![AWS Cloud9][Cloud9-badge]][Cloud9-url]
- [![Amazon S3][S3-badge]][S3-url]
- [![AWS Glue][Glue-badge]][Glue-url]
- [![Amazon Athena][Athena-badge]][Athena-url]
- [![AWS IAM][IAM-badge]][IAM-url]

### Supplementary Tools Used

- [![Pandas][Pandas-badge]][Pandas-url] Reading and storing .CSV
- [![Apache][Apache-badge]][Apache-url] Converting to .parquet
- [![Coursera][Coursera-badge]][Coursera-url] Markdown Cheatsheet
- [![GitHub][GitHub-badge]][GitHub-url] Readme.MD template and Capstone Project Directory

---

## Task 1: Development Environment Setup
### 1.1 Environment and Storage Setup

An AWS Cloud9 environment named **CapstoneIDE** was created using `EC2 instance`. This environment serves as the main workspace for data inspection and transformation.

Two Amazon S3 buckets were created in `us-east-1` region. One bucket stores Parquet-formatted source data, and the other stores query results generated by Amazon Athena.
- `data-source-20031`
- `query-results-20031`

### Download and Inspect Source Data

Three CSV files from a public fisheries dataset were downloaded into the Cloud9 environment. The global file contains over 560,000 records covering yearly fishing activity from 1950 to 2018.

| Dataset file                     | Command                              | Row count |
|----------------------------------|--------------------------------------|-----------|
| SAU-GLOBAL-1-v48-0.csv            | `wc -l SAU-GLOBAL-1-v48-0.csv`        | 561,675   |
| SAU-EEZ-242-v48-0.csv         | `wc -l SAU-EEZ-242-v48-0.csv`     | 2,343     |
| SAU-HighSeas-71-v48-0.csv         | `wc -l SAU-HighSeas-71-v48-0.csv`     | 26,720    |

<details>
  <summary>SAU-GLOBAL-1-v48-0.csv overview (click to expand)</summary>

| year | fishing_entity | fishing_sector | catch_type | reporting_status | gear_type        | end_use_type               | tonnes        | landed_value |
|-----:|----------------|----------------|------------|------------------|------------------|----------------------------|---------------|--------------|
| 1950 | Albania        | Industrial     | Discards   | Unreported       | bottom trawl     | Direct human consumption   | 145.001323158 | 212571.94    |
| 1950 | Albania        | Industrial     | Discards   | Unreported       | pelagic trawl    | Direct human consumption   | 0.986312223   | 1445.93      |
| 1950 | Albania        | Industrial     | Discards   | Unreported       | purse seine      | Direct human consumption   | 0.472828525   | 693.17       |
| 1950 | Albania        | Industrial     | Landings   | Reported         | bottom trawl     | Direct human consumption   | 742.852932541 | 1089022.40   |
| 1950 | Albania        | Industrial     | Landings   | Reported         | bottom trawl     | Other                      | 0.743596529   | 1090.11      |

</details>

### 1.2 Data Transformation

Required Python libraries were installed in the environment. The GLOBAL CSV dataset was read into pandas and converted to Apache Parquet format to improve storage efficiency and query performance.
```Python
  sudo pip3 install pandas pyarrow fastparquet
```

<details>
  <summary>convert GLOBAL dataset to Parquet</summary>

```Python
# Start the python interactive shell 
python3
# Use pandas to convert the file to parquet format
import pandas as pd
df = pd.read_csv('SAU-GLOBAL-1-v48-0.csv')
df.to_parquet('SAU-GLOBAL-1-v48-0.parquet')
exit()
```
</details> 

### 1.3 Upload for Analysis

The Parquet files were uploaded to the S3 source bucket, making the data available for cataloging with AWS Glue and querying with Amazon Athena.

```bash
#Upload to the bucket
aws s3 cp SAU-GLOBAL-1-v48-0.parquet s3://data-source-20031/
```

## Task 2: AWS Glue Crawler and Multi-File Querying
### 2.1 Prepare and Upload High Seas Data

The High Seas CSV dataset was converted to Apache Parquet format using Python and uploaded to the data-source-20031. This ensured consistent storage format across all datasets before cataloging.

```bash
#Upload to the bucket
aws s3 cp SAU-HighSeas-71-v48-0.parquet s3://data-source-20031/
```

<details>
  <summary>Sample rows from SAU High Seas dataset</summary>

| area_name                 | area_type | year | scientific_name                        | common_name                    | functional_group                     | commercial_group        | fishing_entity | fishing_sector | catch_type | reporting_status | gear_type  | end_use_type               | tonnes       | landed_value |
|---------------------------|-----------|------|----------------------------------------|--------------------------------|--------------------------------------|-------------------------|----------------|----------------|------------|------------------|------------|----------------------------|--------------|--------------|
| Pacific, Western Central  | high_seas | 1950 | Marine fishes not identified            | Marine fishes nei              | Medium demersals (30–89 cm)           | Other fishes & inverts  | Japan          | Industrial     | Discards   | Unreported       | longline   | Discards                   | 908.3175636  | 1,331,593.55 |
| Pacific, Western Central  | high_seas | 1950 | Marine fishes not identified            | Marine fishes nei              | Medium demersals (30–89 cm)           | Other fishes & inverts  | USA            | Industrial     | Discards   | Unreported       | longline   | Discards                   | 19.2128353   | 28,166.02    |
| Pacific, Western Central  | high_seas | 1950 | Marine pelagic fishes not identified    | Pelagic fishes                 | Medium pelagics (30–89 cm)            | Other fishes & inverts  | Japan          | Industrial     | Discards   | Unreported       | longline   | Discards                   | 3.8182341    | 5,597.53     |
| Pacific, Western Central  | high_seas | 1950 | Marine pelagic fishes not identified    | Pelagic fishes                 | Medium pelagics (30–89 cm)            | Other fishes & inverts  | Philippines    | Industrial     | Landings   | Reported         | hand lines | Direct human consumption   | 96.9876282   | 142,183.86   |
| Pacific, Western Central  | high_seas | 1950 | Scombridae                              | Mackerels, tunas, bonitos      | Medium pelagics (30–89 cm)            | Perch-likes             | Japan          | Industrial     | Discards   | Unreported       | longline   | Discards                   | 143.5574758  | 210,455.26   |

</details>


### 2.2 Create Glue Database and Run Crawler

A Glue database was created to store dataset metadata. A Glue crawler was then configured to scan the S3 source bucket and run on demand. The crawler inferred schemas automatically and created a single unified table in the Glue Data Catalog.

- `fishdb`
- `fishcrawler`

### 2.3 Configure Athena and Verify Schema Integration

Amazon Athena was configured to store query results in S3 bucket. Querying distinct values in the `area_name` column verified schema integration, confirming that High Seas records contained region values while Global records appeared with NULL values.

```SQL
SELECT DISTINCT area_name FROM fishdb.data_source_20031;
```
**Results**: Pacific, Western Central

### 2.4 Query and Persist Analytical Results

SQL queries were executed in Athena to calculate Fiji’s fishing value in the Pacific, Western Central high seas and across all high seas areas. The final aggregation query was saved as a reusable Athena view for later analysis.

<details>
  <summary>Query: Fiji fishing value in the Pacific, Western Central high seas</summary>
  
```SQL
SELECT year,
       fishing_entity AS Country,
       CAST(CAST(SUM(landed_value) AS DOUBLE) AS DECIMAL(38,2)) AS ValuePacificWCSeasCatch
FROM fishdb.data_source_20031
WHERE area_name LIKE '%Pacific%'
  AND fishing_entity = 'Fiji'
  AND year > 2000
GROUP BY year, fishing_entity
ORDER BY year;
```
</details>

<details>
  <summary>Query: Fiji fishing value across all high seas</summary>
  
```SQL
SELECT year,
       fishing_entity AS Country,
       CAST(CAST(SUM(landed_value) AS DOUBLE) AS DECIMAL(38,2)) AS ValueAllHighSeasCatch
FROM fishdb.data_source_20031
WHERE fishing_entity = 'Fiji'
  AND year > 2000
  AND area_name IS NULL
GROUP BY year, fishing_entity
ORDER BY year;
```
</details>

<details>
  <summary>Query: Create Athena view for all high seas fishing by Fiji</summary>
  
```SQL
CREATE OR REPLACE VIEW challenge AS
SELECT year,
       fishing_entity AS Country,
       CAST(CAST(SUM(landed_value) AS DOUBLE) AS DECIMAL(38,2)) AS ValueAllHighSeasCatch
FROM fishdb.data_source_20031
WHERE fishing_entity = 'Fiji'
  AND year > 2000
  AND area_name IS NULL
GROUP BY year, fishing_entity;
```
</details>

| year | Country | ValueAllHighSeasCatch |
|------|---------|-----------------------|
| 2005 | Fiji    | 70,515,457.92         |
| 2014 | Fiji    | 64,079,863.14         |
| 2016 | Fiji    | 66,248,609.46         |


## Task 3: Schema Transformation and EEZ Data Integration

### 3.1 Identify Schema Differences

The EEZ dataset was inspected to compare its column structure with existing
datasets. Two mismatches were identified, where column names differed despite
containing equivalent data.

| EEZ Column | Unified Column |
|-----------|----------------|
| `country` | `fishing_entity` |
| `fish_name` | `common_name` |

### 3.2 Align Schema and Convert Format

The original EEZ file was backed up and loaded into Python. Column names were
renamed to match the unified schema used by the other datasets. The transformed
file was then saved in both CSV and Parquet formats.

```Python
# Make a backup of the file before you modify it in place
cp SAU-EEZ-242-v48-0.csv SAU-EEZ-242-v48-0-old.csv

# Start the python interactive shell
python3
import pandas as pd

# Load the backup version of the file
data_location = 'SAU-EEZ-242-v48-0-old.csv'

# Use Pandas to read the CSV into a dataframe
df = pd.read_csv(data_location)

# View the current column names
print(df.head(1))

# Change the names of the 'fish_name' and 'country' columns to match the column names where this data appears in the other data files already in your data-source bucket
df.rename(columns = {"<FMI_1>": "<FMI_2>", "<FMI_3>": "<FMI_4>"}, inplace = True)

# Verify the column names have been changed
print(df.head(1))

# Write the changes to disk
df.to_csv('SAU-EEZ-242-v48-0.csv', header=True, index=False)
df.to_parquet('SAU-EEZ-242-v48-0.parquet')
exit()
```
This step ensured that AWS Glue would merge the EEZ data into the existing table
rather than creating separate schema variations.

### 3.3 Upload for Cataloging

The transformed EEZ Parquet file was uploaded to the S3 source bucket, making it
available for cataloging by the Glue crawler and for querying in Athena.

```bash
#Upload to the bucket
aws s3 cp SAU-EEZ-242-v48-0.parquet s3://data-source-20031/
```

```SQL
SELECT DISTINCT area_name FROM fishdb.data_source_20031;
```
**Results**: Fiji, Pacific, Western Central

## Task 4: Advanced Analytical Queries

### 4.1 Top Fishing Nations Analysis

A query was executed to identify the world's largest fishing nations based on total catch volume from 2015 to 2020. This analysis provides insight into which countries dominate global fishing activities in the most recent data period.

<details>
  <summary>Query: Top 10 countries by total catch (2015-2020)</summary>
  
```SQL
SELECT 
    fishing_entity AS Country,
    CAST(CAST(SUM(tonnes) AS DOUBLE) AS DECIMAL(38,2)) AS TotalCatch
FROM fishdb.data_source_20031
WHERE year BETWEEN 2015 AND 2020
GROUP BY fishing_entity
ORDER BY TotalCatch DESC
LIMIT 10;
```
</details>

| Country              | TotalCatch |
|----------------------|------------|
| China                | 62,836,103.85 |
| Russian Federation   | 32,680,642.76 |
| Indonesia            | 26,551,075.16 |
| USA                  | 26,512,786.71 |
| Peru                 | 22,435,470.08 |
| Thailand             | 19,211,236.70 |
| Japan                | 16,788,679.49 |
| India                | 16,226,591.35 |
| Viet Nam             | 15,286,016.69 |
| Mexico               | 10,020,076.17 |

This query aggregates catch data across the five-year period to reveal the countries with the highest fishing output, enabling comparison of fishing capacity between nations.

### 4.2 Fishing Transparency Analysis

To assess the transparency and reporting compliance in global fisheries, a temporal analysis was performed comparing reported versus unreported fishing activities. The query tracks both catch volume and economic value from 2015 onwards.

<details>
  <summary>Query: Reported vs unreported fishing trends</summary>
  
```SQL
SELECT 
    year,
    reporting_status AS Status,
    CAST(CAST(SUM(tonnes) AS DOUBLE) AS DECIMAL(38,2)) AS TotalCatch,
    CAST(CAST(SUM(landed_value) AS DOUBLE) AS DECIMAL(38,2)) AS TotalValue
FROM fishdb.data_source_20031
WHERE year >= 2015
GROUP BY year, reporting_status
ORDER BY year, reporting_status;
```
</details>

| Year | Status      | TotalCatch | TotalValue |
|------|-------------|------------|------------|
| 2015 | Reported    | 81,318,908.70 | 179,791,113,154.10 |
| 2015 | Unreported  | 28,409,678.37 | 41,650,148,769.72 |
| 2016 | Reported    | 77,840,187.47 | 193,958,143,421.51 |
| 2016 | Unreported  | 28,309,406.58 | 41,502,029,723.02 |
| 2017 | Reported    | 80,455,909.32 | 192,038,474,822.16 |
| 2017 | Unreported  | 28,003,809.55 | 41,054,170,817.34 |
| 2018 | Reported    | 81,512,376.46 | 201,891,386,794.57 |
| 2018 | Unreported  | 28,363,186.47 | 41,581,110,401.44 |

The results reveal year-over-year trends in reporting compliance, providing critical data for evaluating the effectiveness of international fishing regulations and monitoring systems.

### 4.3 High-Value Species Identification

An economic analysis was conducted to identify the most valuable fish species in recent years. This query ranks species by their total landed value while also displaying catch volumes for context.

<details>
  <summary>Query: Top 15 most valuable fish types from 2015</summary>
  
```SQL
SELECT 
    common_name AS FishType,
    CAST(CAST(SUM(landed_value) AS DOUBLE) AS DECIMAL(38,2)) AS TotalValue,
    CAST(CAST(SUM(tonnes) AS DOUBLE) AS DECIMAL(38,2)) AS TotalCatch
FROM fishdb.data_source_20031
WHERE common_name IS NOT NULL
  AND year >= 2015
GROUP BY common_name
ORDER BY TotalValue DESC
LIMIT 15;
```
</details>

| FishType                     | TotalValue | TotalCatch |
|------------------------------|------------|------------|
| Skipjack tuna                | 247,303,390,853.56 | 1,105,777.76 |
| Yellowfin tuna               | 46,997,183,280.88  | 424,585.51   |
| Bigeye tuna                  | 2,567,271,851.41   | 98,384.93    |
| Albacore                     | 2,312,970,322.11   | 65,725.04    |
| Marine fishes nei            | 106,159,887.45     | 30,040.20    |
| Swordfish                    | 85,018,772.80      | 12,633.23    |
| Emperors, scavengers         | 84,641,562.21      | 18,399.82    |
| Pacific bluefin tuna         | 54,729,378.30      | 4,129.53     |
| Blue shark                   | 52,114,117.20      | 35,551.02    |
| Aquatic invertebrates        | 31,462,447.62      | 11,218.12    |
| Mackerels, tunas, bonitos    | 27,717,368.07      | 16,190.51    |
| Clams                        | 21,084,940.23      | 9,183.53     |
| Sharks, rays, skates         | 19,034,920.42      | 12,983.13    |
| Pelagic fishes               | 18,733,844.38      | 23,756.23    |
| Triggerfishes                | 10,986,327.22      | 6,628.98     |


This analysis enables identification of commercially important species that drive the fishing industry's economic output.

### 4.4 Analytical Insights

These queries collectively provide:

**Geopolitical Context:** Understanding which nations control fishing resources and capacity

**Regulatory Effectiveness:** Tracking compliance with international reporting requirements

**Economic Drivers:** Identifying species that generate the highest commercial value

## Conclusion

This capstone demonstrated an end-to-end data engineering workflow on AWS using
historical fisheries data from 1950 to 2018. Data from open seas, regional high
seas, and Exclusive Economic Zones was successfully unified into a single,
queryable structure.

The solution transformed over 560,000 records from multiple schemas, enabled
efficient analytical querying through Athena, and supported reusable views for
ongoing analysis. Automated cataloging with AWS Glue ensured consistent metadata
and reliable schema management.

Overall, the architecture is scalable, cost-efficient, and suitable for
large-scale analytical workloads.

<!-- References -->
[Cloud9-badge]: https://img.shields.io/badge/AWS%20Cloud9-232F3E?style=for-the-badge&logo=amazonaws&logoColor=white
[Cloud9-url]: https://aws.amazon.com/cloud9/

[S3-badge]: https://img.shields.io/badge/Amazon%20S3-569A31?style=for-the-badge&logo=amazons3&logoColor=white
[S3-url]: https://aws.amazon.com/s3/

[Glue-badge]: https://img.shields.io/badge/AWS%20Glue-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white
[Glue-url]: https://aws.amazon.com/glue/

[Athena-badge]: https://img.shields.io/badge/Amazon%20Athena-5A4FCF?style=for-the-badge&logo=amazonaws&logoColor=white
[Athena-url]: https://aws.amazon.com/athena/

[IAM-badge]: https://img.shields.io/badge/AWS%20IAM-DD344C?style=for-the-badge&logo=amazonaws&logoColor=white
[IAM-url]: https://aws.amazon.com/iam/

<!-- SUPPLEMENTARY TOOL BADGES -->

[Pandas-badge]: https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white
[Pandas-url]: https://pandas.pydata.org/

[Apache-badge]: https://img.shields.io/badge/Apache-D42029?style=for-the-badge&logo=apache&logoColor=white
[Apache-url]: https://www.apache.org/

<!-- PLATFORM BADGES -->

[Coursera-badge]: https://img.shields.io/badge/Coursera-0056D2?style=for-the-badge&logo=Coursera&logoColor=white
[Coursera-url]: https://www.coursera.org/resources/markdown-cheat-sheet

[GitHub-badge]: https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white
[GitHub-url]: https://github.com/othneildrew/Best-README-Template/blob/main/README.md

